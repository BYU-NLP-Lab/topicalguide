%LaTeX source for the ACL 2011 Demo submission for the Topical Guide project
%TODO: Read topic naming papers

%In case we change the name of the build script, we define a macro for it:
\newcommand{\buildscript}{backend.py}
\newcommand{\tool}{Topical Guide}
\newcommand{\projecturl}{http://nlp.cs.byu.edu/topicalguide}

\documentclass[11pt]{article}
\usepackage{acl-hlt2011}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\usepackage{graphicx}
\usepackage{listings}	
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Increased Comprehension of Topic Models and Corpora Using the \tool}
\author{Josh Hansen, Matthew J. Gardner, Jeff Lund, Dan Walker, Eric Ringger, \and Kevin Seppi\\
Department of Computer Science\\
Brigham Young University\\
\tt \{mjg82,jlutes,joshhansen\}@byu.edu, \{jefflund,danwalkeriv\}@gmail.com\\
\tt \{ringger,kseppi\}@cs.byu.edu}

\begin{document}
\maketitle

\begin{abstract}
Though topic models reduce dimensionality, their output is just as large as the training corpus.
Thus, humans' ability to manually assess model quality is limited. As digitization of historic texts
proceeds at breakneck pace, institutions correspondingly find an increased need for tools such as
topic models to make these texts usable. In response to these twin needs, we present the \tool,
an open-source\footnote{Licensed under the terms of the
Affero General Public License, version 3.} web application for interactive, topic-centric
exploration and visualization of topic model output.
We explain why such a tool is warranted, what it is capable of, and
how to use it to explore the corpus or topic model of your choice.
\end{abstract}

\section{Introduction}
Since its introduction, LDA-based topic modeling \cite{blei_latent_2003} has
become standard fare for those wishing to automatically distill large text
collections into something more immediately useful to humans and computers
alike. The usefulness of this sort of dimensionality reduction is widely
acknowledged, and topic models continue to be extended into exciting new
territory.
\cite{wang_continuous_2008,mimno_polylingual_2009,boyd-graber_holistic_2010,brody_unsupervised_2010,he_detecting_2009,yao_efficient_2009}
However, these models output a topic assignment per token, resulting in an
output that is a corpus unto itself. Though the range of possible values is
substantially reduced, output size remains on the same order as input size.
Notwithstanding, papers introducing new topic models often lean heavily on the
hope that displaying a few hand-picked topics will convince others of the
model's effectiveness. But the very vastness of topic model output renders this
sort of presentation unconvincing. Unfortunately, tools for deep understanding
of topic model output have thus far been lacking.

Simultaneously, massive digitization efforts by libraries and other institutions
are generating gargantuan quantities of electronic text, increasing demand for tools such as
topic models. Because of their ability to characterize the contents of
corpora, topic models seem to hold great promise for archival
institutions wishing to make their newly-digitized collections accessible to
researchers and the public. However, readily available tools for exploration of 
document collections in terms of topic models have been either unsatisfactory
or nonexistent.

We present the \tool, an open-source web application for interactive,
topic-aware exploration and visualization of both document collections and the
topic models inferred on them. Further information on the project, including
source code access and a live demonstration server, can be found
at \texttt{\projecturl}.

Throughout this paper, examples are taken from a
corpus of State of the Union messages delivered by United States presidents
from 1790 to 2010. This constitutes 223 documents totalling XXXXXXX tokens. %TODO Count tokens

\begin{figure*}[t]
 \centering
 \includegraphics[width=400px,keepaspectratio=true]{./topic_page_take2.png}
 % topic_page_huge_cropped.png: 1394x1027 pixel, 96dpi, 36.88x27.17 cm, bb=0 0 1045 770
 \caption{The topic page}
 \label{fig:topic_page}
\end{figure*}

%TODO Segue

\section{Browsing}%TODO Talk about more features
The \tool{} object model and user interface include notions of
\textit{datasets} and \textit{analyses}.
A dataset is a set of documents --- a corpus. An analysis is associated with one
dataset and constitutes a set of topics and token-level topic assignments for
all of the documents in that dataset. The analysis concept is useful in
allowing for multiple topic models on a single dataset. For example,
it could be useful to use LDA to infer both a 20-topic model and a 100-topic model
on the State of the Union Addresses dataset.

All entities explicitly modeled by a basic topic model --- topics, documents, and
words --- are first-class citizens in the \tool, meaning that the user
interface provides specific views for each of these entity types. For example, a
view of the ``government federal'' topic is rendered in Figure \ref{fig:topic_page}.
It displays statistics about the topic (``STATS''); chart, word-cloud, and
key-word-in-context representations of top words (``TOP WORDS $P(W|Z)$''/
``WORD CLOUD''/``TOP WORDS IN CONTEXT''); and both textual and graphical
representations of similar topics (``SIMILAR TOPICS''/``TOPIC MAP'').

Intuitive hyperlinks make this a true \textit{browsing} tool. Click on the
``government states'' node in the topic map and be sent to its topic view to see
how presidents have discussed state government. Click on ``federal'' in the word
cloud to see the word view, which shows top topics and documents in which the
word occurs. Or click on ``Grover\_\allowbreak{}Cleveland's\_\allowbreak{}Fourth\_\allowbreak{}State\_\allowbreak{}of\_\allowbreak{}the\_\allowbreak{}Union\_\allowbreak{}Address.txt''
to see President Cleveland's message with words belonging to the ``government federal''
topic highlighted.\footnote{For a more complete discussion
of browsing features see \cite{gardner_browser_2010}.}%TODO Replace with actual discussion
%TODO Make words in the doc view link
%TODO Show words in context in the word view

\section{Document Metadata}
Acknowledging that more is known about most documents than just their content,
the \tool{} provides \textit{document attributes}, a mechanism for association
of arbitrary metadata with documents. This accomodates metadata
provided by a dataset curator, often including facts about document
provenance and author identity. Document attributes also allow for metadata
obtained as part of the output of some model.

\section{Metrics}
Topic-centric document exploration is enhanced by means of \textit{metrics}.
Metrics are functions that give users additional insight into the nature of
topics or documents. They can be either unary or binary, here called
\textit{pairwise}.\footnote{Pairwise metrics do not necessarily constitute
``metrics'' in the formal sense. We leave it to implementers to determine
whether to satisfy the triangle inequality, etc.} Topic metrics range from
simple metrics, such as the number of word tokens and types labeled with the
topic, to more complicated metrics such as dispersion across documents,
prevailing sentiment, or the semantic coherence of its words \cite{Newman2010}.
Pairwise topic metrics include Pearson correlation on documents and on words.
These are used to automatically display a list of similar topics and to weight
the edges of the graph used to generate ``topic maps'' (discussed further in
\ref{subsec:maps}). Similar to topic metrics, one can also compute document
metrics. Beyond simple metrics like to- ken count in the document, these include
things such as the entropy of the topic distribution of the document [8]. And as
with topics, we make use of pairwise document metrics such as topic%TODO: Cite
correlation [3] to show similar documents.%TODO: Cite

\section{Charts}

\begin{figure*}[t]
 \centering
 \includegraphics[height=200px,keepaspectratio=true]{./topics_vs_years.png}
 % topics_vs_years.png: 979x520 pixel, 96dpi, 25.90x13.76 cm, bb=0 0 734 390
 \label{fig:chart}
 \caption{Selected topics over time as a percentage of overall tokens.}
\end{figure*}
%TODO: Cite PNAS2005, ToT

\section{Topic ``Maps"}\label{subsec:maps}
With topic-to-topic relationships described by means of pairwise topic metrics,
graph-based visualization of the topic space becomes straightforward. In our
current implementation, we construct a topic graph $G = (N, E)$ as follows:
$N$ is a set of $|T|$ nodes such that
\[\forall_{t\in T} weight(N_{t}) = \tau_{1}(t)\]
and
\[\forall_{t\in T} color(N_{t}) \sim \tau_{2}(t)\]
where $\tau_1$ and $\tau_2$ are topic metrics (potentially the same). $E$ is
constructed as a set of $|T|^2$ edges such that
  \[\forall_{(t,u)\in T\times T, t\neq u} weight(E_{t,u}) = \pi(t,u)\]
where $\pi$ is a pairwise topic metric.

We use the Gephi Toolkit\footnote{http://www.gephi.org} to generate such graphs
and render them as images, employing the \texttt{ForceAtlas} force-directed
layout algorithm to arrange the nodes so that, generally speaking, nodes joined
by edges of higher weight are closer together, and nodes joined by edges of
lower weight are further apart. In the final rendering of the image, edges
are omitted to reduce visual complexity. However, the distances between nodes
are still determined by the interaction of the layout algorithm and the edge
weights.

%TODO Callback

\section{Topic Name Schemes}
As LDA does not assign names to the topics it generates, automatic generation of
topic names is of interest to researchers wishing to make topic models
human-usable. Research in this area is ongoing \cite{Mei2007,Lau2010}. In order to
facilitate investigations in this area, we equipped the \tool{} with a
fully pluggable topic naming system. Any number of topic name schemes can be
used to produce names for all topics in an analysis. Within the user interface
users can select a name scheme, which is then reflected throughout the
interface. By default we use a concatenation of the two
words with the highest probability for a given topic $z$, but any number of
alternative schemes can be imagined and implemented.%TODO Maybe show TF-ITF?

\section{Data Import Backend}
\begin{figure*}[t]
 \centering
 \includegraphics[width=400px,keepaspectratio=true]{./build_flowchart.png}
 % build_deps.png: 1188x340 pixel, 72dpi, 41.91x11.99 cm, bb=0 0 1188 340
 \caption{Dependencies within the data import pipeline. Arrows indicate that the source is prerequisite for the target.}
 \label{fig:build_flowchart}
\end{figure*}

Automatically turning a raw document collection into an interactive browsing
experience requires extensive preprocessing. Documents must be converted into a
representation accepted by the topic model learner. Topics must then be inferred
and model output indexed for quick retrieval by the web front-end.
Additionally, metrics must be computed, topic names generated, and graphs rendered
before they become accessible via the user interface. Unsurprisingly, the dependencies
in the import process form a directed acyclic graph, represented in Figure \ref{fig:build_flowchart}. To
improve the efficiency and usability of the data import pipeline we implemented
the import process in the form of a command-line frontend based on the \texttt{doit}
task automation tool.\footnote{http://doit.sourceforge.net/} This allows full
import of a dataset using a single invocation of the import script.

Enabling the \tool{} to browse a new dataset can be as simple as picking a name and description,
converting the documents to plaintext and placing them in a directory, and
providing a JSON file containing document metadata. The \tool{} will automatically
invoke MALLET's LDA implementation \cite{McCallum2002}. The import system can
also make use of existing topic model output in the MALLET format.

\section{Conclusion}
In this paper we introduced the \tool{} as a tool for exploration of
topic model output. This serves to improve understanding of both topic models
and text corpora. We briefly described some of the general features of the tool
and proceeded to detail the use of metrics and graphs. We then explained how
new datasets can easily be imported into the tool.

\bibliographystyle{acl}
\bibliography{acl2011demo.bib}
\end{document}

